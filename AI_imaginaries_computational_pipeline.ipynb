{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook provides end-to-end code for using the computational pipeline based on BERT-NLI approach from the paper \"[Building Efficient Universal Classifiers with Natural Language Inference](https://arxiv.org/abs/2312.17543)\" by Moritz Laurer, Wouter van Atteveldt, Andreu Casas, Kasper Welbers (2024). I developed this pipeline as part of my master’s thesis. The pipeline is designed to analyze Sociotechincal Imaginaries of AI in Italian online news websites and combines NLI-based zero-shot classification of the articles with topic modeling using BERTopic (Grootendorst, 2022 [link text](https://maartengr.github.io/BERTopic/index.html)). After text preprocessing, the pipeline is divided into two phases, each consisting of two steps. The piepline is grounded in the SIPCs (Sociotechnical Imaginaries in Public Communication) framework developed by Brause et al. (2025) [link text](https://journals.sagepub.com/doi/full/10.1177/13548565251338192). To apply the pipeline to news media in other languages, it is sufficient to adapt the text preprocessing and translate the hypotheses for NLI classification."
      ],
      "metadata": {
        "id": "y1xwSF5621DX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Activate a GPU runtime"
      ],
      "metadata": {
        "id": "JuTHcUwb5Qj_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to run this notebook on a GPU, click on \"Runtime\" > \"Change runtime type\" > select \"GPU\" in the menue bar in to top left. Executing the notebook on a GPU allows for much faster analysis."
      ],
      "metadata": {
        "id": "OZoAtCy_MhnU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install relevant packages"
      ],
      "metadata": {
        "id": "VOeUg4_YpxC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[sentencepiece]\n",
        "!pip install bertopic[all] sentence-transformers"
      ],
      "metadata": {
        "id": "7H9y8Pwr55_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data download and article preprocessing"
      ],
      "metadata": {
        "id": "dfhUvES762KO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset comprises Italian-language articles sourced from news websites. Articles whose headlines contain “IA” or “intelligenza artificiale” were collected over a five-year period. The dataset is not publicly available due to privacy constraints. This does not affect the pipeline architecture presented below, which can be readily adapted to Italian newspaper corpora for the analysis of AI imaginaries"
      ],
      "metadata": {
        "id": "dXara78PwY0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#loading the dataset\n",
        "#Let us assume that the dataset has several columns, such as \"Author\", \"Date\" etc. The column containing the article text is called \"Content\".\n",
        "#\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "drive.mount('/content/drive')\n",
        "df_articles = pd.read_csv ('path to your dataset') #Enter the path to your dataset in the drive here\n",
        "\n",
        "#Minimal preprocessing of the articles (can be adapted and expanded depending on the dataset's requirement)\n",
        "# Remove location markers at the beginning of the articles if they appear within the first ~30 characters\n",
        "\n",
        "def clean_italian_articles(text):\n",
        "    \"\"\"\n",
        "    Clean Italian news articles by removing common metadata and stamps\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    text = str(text).strip()\n",
        "\n",
        "    if ' - ' in text[:30]:\n",
        "        parts = text.split(' - ', 1)\n",
        "        if len(parts[0]) < 25:\n",
        "            text = parts[1].strip()\n",
        "    if ' – ' in text[:30]:\n",
        "        parts = text.split(' – ', 1)\n",
        "        if len(parts[0]) < 25:\n",
        "            text = parts[1].strip()\n",
        "\n",
        "    #Remove author names at the end of the articles if they are in parentheses\n",
        "    if text.endswith(')'):\n",
        "        last_paren = text.rfind('(')\n",
        "        if last_paren != -1:\n",
        "            author_part = text[last_paren+1:-1].strip()\n",
        "            if len(author_part) < 50 and author_part.replace(' ', '').replace('.', '').isalpha():\n",
        "                text = text[:last_paren].strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply the cleaning function to the Content column\n",
        "df_articles['Content'] = df_articles['Content'].apply(clean_italian_articles)\n",
        "\n",
        "# Formatting the articles to facilitate classification using NLI models\n",
        "\n",
        "# Add new column 'Content_formatted' with the quote structure\n",
        "\n",
        "df_articles['Content_formatted'] = df_articles['Content'].apply(\n",
        "    lambda text: f'La citazione: \"{text}\" - fine della citazione.'\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yTJT4bSH636Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First phase step 1 - NLI-based classification of the articles according to the vision dimension"
      ],
      "metadata": {
        "id": "pXGk6aHcIMnv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first phase aims to define the envisioned roles of AI, which form the foundation of imaginaries. The first step consists of classifying the articles through NLI according to the Vision dimension, within the SIPCs framework (Brause et al., 2025 [link text](https://journals.sagepub.com/doi/10.1177/13548565251338192)). The multilingual model \"MoritzLaurer/bge-m3-zeroshot-v2.0\" is used for zero-shot classification, since its 8,192-token context window enables full processing of newspaper texts. Depending on the specific requirements, it is possible to select a zero-shot classifier from Hugging Face (https://huggingface.co/collections/MoritzLaurer/zeroshot-classifiers-6548b4ff407bb19ff5c3ad6f)"
      ],
      "metadata": {
        "id": "SUK012AmIZem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load the multilingual zero-shot classification pipeline from Hugging Face\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "device = 0 if torch.cuda.is_available() else -1  # GPU=0, CPU=-1\n",
        "classifier = pipeline(\n",
        "    \"zero-shot-classification\",\n",
        "    model=\"MoritzLaurer/bge-m3-zeroshot-v2.0\",\n",
        "    framework=\"pt\",\n",
        "    device=device\n",
        ")\n",
        "\n",
        "\n",
        "# Define hypothesis template and candidate labels\n",
        "hypothesis_template = \"La citazione contiene riferimenti a {}\"\n",
        "candidate_labels = [\n",
        "    \"il ruolo esplicito o l'uso specifico che l'intelligenza artificiale o una sua tecnologia avrà in futuro\"\n",
        "]\n",
        "\n",
        "# Apply classification with batch processing for speed\n",
        "texts = df_articles[\"Content_formatted\"].tolist()\n",
        "batch_size = 8\n",
        "all_scores = []\n",
        "\n",
        "for i in range(0, len(texts), batch_size):\n",
        "    batch_texts = texts[i:i+batch_size]\n",
        "    batch_results = classifier(\n",
        "        batch_texts,\n",
        "        candidate_labels,\n",
        "        hypothesis_template=hypothesis_template,\n",
        "        multi_label=True\n",
        "    )\n",
        "    batch_scores = [result[\"scores\"][0] for result in batch_results]\n",
        "    all_scores.extend(batch_scores)\n",
        "\n",
        "# Store results\n",
        "df_articles[\"probability_score\"] = all_scores\n",
        "df_articles[\"Role\"] = (df_articles[\"probability_score\"] > 0.5).astype(int)\n"
      ],
      "metadata": {
        "id": "YVtMiC1IIKlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First phase step 2 - Topic modeling with BERTopic to identify the imaginary contexts and the role of AI within imaginaries"
      ],
      "metadata": {
        "id": "KeOSAyefOGri"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Through a dual clustering approach with BERTopic, combined with the manual reading of a sample of articles, it is possible to analyze the articles previously classified by NLI in order to identify the envisioned role of AI within different imaginaries."
      ],
      "metadata": {
        "id": "shzf0VGHOCcp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, BERTopic is applied to the articles to identify the broader imaginary contexts, namely the thematic domains and areas in which AI imaginaries are situated. \"Qwen/Qwen3-Embedding-0.6B\" is used as the embedding model, since its 8,192-token context window enables full-article processing of newspaper texts."
      ],
      "metadata": {
        "id": "G8V_pPP2PNB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from bertopic import BERTopic\n",
        "from bertopic.representation import KeyBERTInspired\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Download Italian stopwords\n",
        "nltk.download('stopwords', quiet=True)\n",
        "italian_stops = stopwords.words('italian')\n",
        "\n",
        "# Define custom stopwords. Since we already know that all the articles focus on AI, we can add the following stopwords\n",
        "custom_stops = [\"intelligenza\", \"artificiale\", \"AI\", \"IA\"]\n",
        "\n",
        "# Combine standard and custom stopwords into a list\n",
        "all_stopwords = italian_stops + custom_stops\n",
        "\n",
        "# Prepare the list of documents - only Role = 1\n",
        "role_1_articles = df_articles[df_articles['Role'] == 1].copy()\n",
        "docs = role_1_articles['Content'].astype(str).tolist()\n",
        "\n",
        "print(f\"Total articles: {len(df_articles)}\")\n",
        "print(f\"Role = 1 articles: {len(role_1_articles)}\")\n",
        "print(f\"Documents to analyze: {len(docs)}\")\n",
        "\n",
        "#Work with GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load the embedding model\n",
        "embedding_model = SentenceTransformer(\n",
        "    \"Qwen/Qwen3-Embedding-0.6B\",\n",
        "    device=device,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Calculate embeddings (conservative batch size)\n",
        "embeddings = embedding_model.encode(\n",
        "    docs,\n",
        "    batch_size=1,\n",
        "    show_progress_bar=True,\n",
        "    convert_to_numpy=True,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# Create a CountVectorizer that filters out our stopwords\n",
        "vectorizer_model = CountVectorizer(stop_words=all_stopwords)\n",
        "\n",
        "# Instantiate BERTopic with custom vectorizer and representation model\n",
        "topic_model = BERTopic(\n",
        "    vectorizer_model=vectorizer_model,  # remove stopwords at vectorization\n",
        "    calculate_probabilities=True        # compute topic probabilities\n",
        ")\n",
        "\n",
        "# Fit the model and transform the documents into topics with batch processing\n",
        "topics, probs = topic_model.fit_transform(docs, embeddings=embeddings)\n",
        "\n",
        "# Store the topic IDs and max probability back into the DataFrame\n",
        "role_1_articles['Topic'] = topics\n",
        "role_1_articles['Topic_Probability'] = [p.max() if p is not None else 0 for p in probs]\n",
        "\n",
        "#Using BERTopic’s visualizations to examine the topics.\n",
        "print(topic_model.get_topic_info())\n",
        "topic_model.visualize_barchart()\n",
        "#In particular, visualizing the hierarchical topics is useful for merging clusters, especially when working with many documents\n",
        "hierarchical_topics = topic_model.hierarchical_topics(docs)\n",
        "topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)\n",
        "\n",
        "# Add \"Imaginary context\" column with the BERTopic topic number\n",
        "df_articles.loc[role_1_articles.index, \"Imaginary context\"] = role_1_articles[\"Topic\"].values"
      ],
      "metadata": {
        "id": "BIPzABhCPQ7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the end of the previous step, a CSV file is produced containing a column with the BERTopic cluster assignments — that is, the imaginary contexts. At this stage, one of these contexts can be selected to identify the role of AI and, subsequently, to analyze the AI imaginary as a whole. As an example, the first imaginary (Imaginary1) contex — corresponding to BERTopic cluster 0 — is used. A second round of clustering with BERTopic is then applied only to the articles within that context, complemented by manual reading of a sample of articles to interpret the results. By the end of this phase, the role of AI within the imaginary. The next phase analyzes these articles to determine the remaining dimensions of the SIPCs framework, thereby providing a comprehensive account of the imaginary."
      ],
      "metadata": {
        "id": "cE2nqkTkeiz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "# Select only articles in Imaginary1\n",
        "imaginary1_articles = df_articles[df_articles[\"Imaginary context\"] == 0].copy()\n",
        "docs_imaginary1 = imaginary1_articles[\"Content\"].astype(str).tolist()\n",
        "\n",
        "# Recompute embeddings for this subset\n",
        "embeddings_imaginary1 = embedding_model.encode(\n",
        "    docs_imaginary1,\n",
        "    batch_size=1,\n",
        "    show_progress_bar=True,\n",
        "    convert_to_numpy=True,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# BERTopic on the selected Imaginary1 context\n",
        "topic_model_imaginary1 = BERTopic(\n",
        "    vectorizer_model=vectorizer_model,\n",
        "    calculate_probabilities=True\n",
        ")\n",
        "topics_imaginary1, probs_imaginary1 = topic_model_imaginary1.fit_transform(docs_imaginary1, embeddings=embeddings_imaginary1)\n",
        "\n",
        "# Store back to the main DataFrame\n",
        "imaginary1_articles[\"Subtopic\"] = topics_imaginary1\n",
        "imaginary1_articles[\"Subtopic_Probability\"] = [p.max() if p is not None else 0 for p in probs_imaginary1]\n",
        "\n",
        "df_articles.loc[imaginary1_articles.index, \"Imaginary subcontext\"] = imaginary1_articles[\"Subtopic\"]\n",
        "df_articles.loc[imaginary1_articles.index, \"Subtopic_Probability\"] = imaginary1_articles[\"Subtopic_Probability\"]\n",
        "\n",
        "# For each subtopic group, randomly sample half of the articles\n",
        "sample_df = imaginary1_articles.groupby('Subtopic').apply(\n",
        "    lambda x: x.sample(n=int(len(x) / 2), random_state=42)\n",
        ").reset_index(drop=True)\n",
        "\n",
        "# Optional: Check how many articles were sampled from each subtopic\n",
        "print(\"Sample size per subtopic:\")\n",
        "print(sample_df['Subtopic'].value_counts())\n",
        "\n",
        "# Export the sampled DataFrame to an Excel file\n",
        "sample_df.to_excel('path', index=False)\n",
        "\n",
        "# Download the Excel file to your computer in order to inspect the clusters and read the articles from the Excel file\n",
        "files.download('path')"
      ],
      "metadata": {
        "id": "g5IsbZ2dekav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Second phase step 1 - NLI-based classification of all remaining dimensions of the imaginary"
      ],
      "metadata": {
        "id": "aalkNuGo4S3U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLI-based classification is applied to identify the remaining dimensions of the SIPCs framework for Imaginary 1 under analysis"
      ],
      "metadata": {
        "id": "hVhAcPXM4lmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#OBJECT DIMENSION\n",
        "\n",
        "# Load the multilingual zero-shot classification pipeline\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "device = 0 if torch.cuda.is_available() else -1  # GPU=0, CPU=-1\n",
        "classifier = pipeline(\n",
        "    \"zero-shot-classification\",\n",
        "    model=\"MoritzLaurer/bge-m3-zeroshot-v2.0\",\n",
        "    framework=\"pt\",\n",
        "    device=device\n",
        ")\n",
        "print(f\"Running inference on: {'cuda:0' if device==0 else 'cpu'}\")\n",
        "\n",
        "# Define hypothesis template and candidate labels\n",
        "hypothesis_template = \"La citazione contiene riferimenti a {}\"\n",
        "candidate_labels = [\"il nome di una specifica tecnologia dell'intelligenza artificiale usata per un determinato scopo\"]\n",
        "\n",
        "# Apply classification with batch processing for speed (Imaginary1 only)\n",
        "texts = imaginary1_articles[\"Content_formatted\"].tolist()\n",
        "batch_size = 8\n",
        "all_scores = []\n",
        "\n",
        "for i in range(0, len(texts), batch_size):\n",
        "    batch_texts = texts[i:i+batch_size]\n",
        "    batch_results = classifier(\n",
        "        batch_texts,\n",
        "        candidate_labels,\n",
        "        hypothesis_template=hypothesis_template,\n",
        "        multi_label=True\n",
        "    )\n",
        "    batch_scores = [result[\"scores\"][0] for result in batch_results]\n",
        "    all_scores.extend(batch_scores)\n",
        "\n",
        "# Add scores to the filtered dataframe (Imaginary1)\n",
        "imaginary1_articles[\"probability_score\"] = all_scores\n",
        "\n",
        "# Create Object dimension column: 1 if probability > 0.5, otherwise 0\n",
        "imaginary1_articles[\"Object dimension\"] = (imaginary1_articles[\"probability_score\"] > 0.5).astype(int)\n",
        "\n",
        "# IMPLICATIONS DIMENSION\n",
        "# Define the hypothesis template\n",
        "hypothesis_template = \"La citazione contiene riferimenti a {}\"\n",
        "\n",
        "# Function to classify each sentence and return the entailment score\n",
        "def classify_futuro(text):\n",
        "    result = classifier(\n",
        "        text,\n",
        "        candidate_labels=[\"raccomandazioni o azioni specifiche, o specifici responsabili per l'uso dell'intelligenza artificiale\"],\n",
        "        multi_label=True\n",
        "    )\n",
        "    return result[\"scores\"][0]\n",
        "\n",
        "# Apply the model to every sentence for Imaginary1 articles\n",
        "probability_scores = imaginary1_articles[\"Content_formatted\"].apply(classify_futuro)\n",
        "\n",
        "# Convert the score to a binary label using a threshold (0.5)\n",
        "imaginary1_articles[\"Implications\"] = (probability_scores >= 0.5).astype(int)\n",
        "\n",
        "# SPATIO-TEMPORAL FUTURES\n",
        "\n",
        "# Define the hypothesis template\n",
        "hypothesis_template = \"La citazione contiene riferimenti a {}\"\n",
        "\n",
        "# Function to classify each sentence and return the entailment score\n",
        "def classify_futuro(text):\n",
        "    result = classifier(\n",
        "        text,\n",
        "        candidate_labels=[\"periodi di tempo in cui l'intelligenza artificiale o una sua tecnologia si svilupperà, oppure luoghi in cui l'intelligenza artificiale o una sua tecnologia si svilupperà.\"],\n",
        "        multi_label=True\n",
        "    )\n",
        "    return result[\"scores\"][0]\n",
        "\n",
        "# Apply the model to Imaginary1 articles\n",
        "probability_scores = imaginary1_articles[\"Content_formatted\"].apply(classify_futuro)\n",
        "\n",
        "# Convert the score to a binary label using a threshold (0.5)\n",
        "imaginary1_articles[\"Spatio_temporal\"] = (probability_scores >= 0.5).astype(int)\n",
        "\n",
        "#(UN)DESIRABILITY DIMENSION\n",
        "\n",
        "# Define the hypothesis template and candidate labels\n",
        "hypothesis_template = \"La citazione contiene riferimenti a {}\"\n",
        "candidate_labels = [\n",
        "    \"effetti positivi e vantaggi dell'uso dell'intelligenza artificiale\",\n",
        "    \"effetti negativi e rischi dell'uso dell'intelligenza artificiale\"\n",
        "]\n",
        "\n",
        "# Classifier already instantiated earlier as `classifier`\n",
        "def classify_desirability(text):\n",
        "    result = classifier(\n",
        "        text,\n",
        "        candidate_labels=candidate_labels,\n",
        "        hypothesis_template=hypothesis_template,\n",
        "        multi_label=True\n",
        "    )\n",
        "    return dict(zip(result[\"labels\"], result[\"scores\"]))\n",
        "\n",
        "# Apply to Imaginary1\n",
        "score_dicts = imaginary1_articles[\"Content_formatted\"].apply(classify_desirability)\n",
        "positive_scores = score_dicts.apply(lambda d: d[candidate_labels[0]])\n",
        "negative_scores = score_dicts.apply(lambda d: d[candidate_labels[1]])\n",
        "\n",
        "# Add columns with correct spelling\n",
        "imaginary1_articles[\"Desirability\"] = (positive_scores >= 0.5).astype(int)\n",
        "imaginary1_articles[\"Undesirability\"] = (negative_scores >= 0.5).astype(int)\n",
        "\n",
        "# STAKEHOLDER AND SPEAKER DIMENSION\n",
        "# Define the hypothesis template and labels\n",
        "hypothesis_template = \"La citazione contiene riferimenti a {}\"\n",
        "labels = {\n",
        "    \"Politics\": \"rappresentanti politici che esprimono dichiarazioni o opinioni sull'intelligenza artificiale\",\n",
        "    \"Industry\": \"rappresentanti del settore industriale, come aziende big tech o altre imprese, che esprimono dichiarazioni o opinioni sull'intelligenza artificiale\",\n",
        "    \"Civil society\": \"membri di ONG o associazioni indipendenti che esprimono dichiarazioni o opinioni sull'intelligenza artificiale\",\n",
        "    \"Academia\": \"membri del settore accademico che esprimono dichiarazioni o opinioni sull'intelligenza artificiale\",\n",
        "    \"Media\": \"giornalisti o operatori dei media che esprimono dichiarazioni o opinioni sull'intelligenza artificiale\"\n",
        "}\n",
        "\n",
        "def classify_row(text):\n",
        "    result = classifier(\n",
        "        text,\n",
        "        candidate_labels=list(labels.values()),\n",
        "        hypothesis_template=hypothesis_template,\n",
        "        multi_label=True\n",
        "    )\n",
        "    reverse_map = {v: k for k, v in labels.items()}\n",
        "    binary_results = {key: 0 for key in labels.keys()}\n",
        "    for label_desc, score in zip(result[\"labels\"], result[\"scores\"]):\n",
        "        if label_desc in reverse_map:\n",
        "            key = reverse_map[label_desc]\n",
        "            binary_results[key] = int(score >= 0.5)\n",
        "    return binary_results\n",
        "\n",
        "# Apply to Imaginary1 articles\n",
        "score_dicts = imaginary1_articles[\"Content_formatted\"].apply(classify_row)\n",
        "binary_predictions = pd.DataFrame(list(score_dicts))\n",
        "\n",
        "# Add columns to imaginary1_articles\n",
        "for col in labels.keys():\n",
        "    imaginary1_articles[col] = binary_predictions[col].astype(int)\n"
      ],
      "metadata": {
        "id": "4BiWziKa4zDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Save and download Imaginary1 with all dimension columns as CSV in order to inspect the results manually and with BERTopic\n",
        "OUTPUT_PATH = \"/content/imaginary1_with_dimensions.csv\"\n",
        "imaginary1_articles.to_csv(path, index=False)\n",
        "files.download('path')"
      ],
      "metadata": {
        "id": "k7rXZEgvDi-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Second phase step 2 - BERTopic and manual reading to analyze articles within each dimension and identify the full set of characteristics of the imaginary"
      ],
      "metadata": {
        "id": "IabGUQatDpxN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERTopic can be applied, as needed, to articles within each dimension to cluster semantically similar texts and facilitate the researcher’s interpretation of the imaginaries. This enables granular analysis of imaginaries and supports a systematic examination of the SIPCs framework’s elements, with manual reading by the researcher guided by NLI filters and BERTopic-based clustering. BERTopic is advisable when the number of articles in a dimension is sufficiently large; otherwise, the researcher can rely on manual reading. As an illustrative case, the code below applies BERTopic to articles that NLI classified as belonging to the Object dimension. The same code can be replicated by substituting the dimension with the one to be analyzed"
      ],
      "metadata": {
        "id": "qxY3dNUtD8Z4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply BERTopic only to articles with Object dimension = 1 (within Imaginary1)\n",
        "\n",
        "# 1) Select subset\n",
        "object_articles = imaginary1_articles[imaginary1_articles[\"Object dimension\"] == 1].copy()\n",
        "docs_object = object_articles[\"Content\"].astype(str).tolist()\n",
        "\n",
        "# 2) Compute embeddings using the existing embedding_model/device\n",
        "embeddings_object = embedding_model.encode(\n",
        "    docs_object,\n",
        "    batch_size=1,\n",
        "    show_progress_bar=True,\n",
        "    convert_to_numpy=True,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# 3) BERTopic on the Object subset\n",
        "topic_model_object = BERTopic(\n",
        "    vectorizer_model=vectorizer_model,\n",
        "    calculate_probabilities=True\n",
        ")\n",
        "object_topics, object_probs = topic_model_object.fit_transform(docs_object, embeddings=embeddings_object)\n",
        "\n",
        "# 4) Store results back\n",
        "object_articles[\"Object subtopic\"] = object_topics\n",
        "object_articles[\"Object subtopic probability\"] = [p.max() if p is not None else 0 for p in object_probs]\n",
        "\n",
        "imaginary1_articles.loc[object_articles.index, \"Object subtopic\"] = object_articles[\"Object subtopic\"]\n",
        "imaginary1_articles.loc[object_articles.index, \"Object subtopic probability\"] = object_articles[\"Object subtopic probability\"]\n",
        "\n",
        "#Using BERTopic’s visualizations to examine the clusters.\n",
        "print(topic_model.get_topic_info())\n",
        "topic_model.visualize_barchart()\n",
        "\n",
        "# For each Object cluster, randomly sample half of the articles\n",
        "sample_object_df = object_articles.groupby('Object subtopic').apply(\n",
        "    lambda x: x.sample(n=int(len(x) / 2), random_state=42)\n",
        ").reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "uWD99pI2HVBs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}